{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from data.dataset import Experience\n",
    "from models.vae import Decoder, Encoder, VAE\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Physics Game\n",
    "I created a simple interactive physics game using Unity 3D. The game contains a fixed size platform and an object, which the agent can interact with. The object also has a set of latent physics properties (mass, dynamic friction, static friction and bounciness) which are not observed by the agent.\n",
    "\n",
    "<img src=\"./physics.png\" width=\"200\">\n",
    "<center><i>Interactive physics game</i></center>\n",
    "\n",
    "The agent can perform an action and receive an observation at each time step:\n",
    "- Action: $a = (f_x, f_y)$, a 2D force vector (parallel to the platform) applied to the object.\n",
    "- Observation: $o = (p_x, p_y, v_x, v_y)$, position and velocity of the object.\n",
    "\n",
    "The object has a latent physics properties which are not observed by the agent. The physics properties are randomly set at the beginning of each interaction session (episode)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent physics inference\n",
    "### Notations\n",
    "- Let $z$ be the latent properties, which is fixed for the object(s) within an episode.\n",
    "- Let $o_t$ be the observed variable at time $t$\n",
    "- Let $a_t$ be the action at time $t$\n",
    "- Interaction history $\\xi_t = (o_1, a_1, o_2,...o_{t-1},a_{t-1}, o_t)$\n",
    "\n",
    "### Belief Update\n",
    "Suppose the agent holds belief $p(z|\\xi_t)$ about $z$ after interaction history $\\xi_t$. Then, the agent makes an action $a_t$ and observes $o_{t+1}$. How should the agent update its belief $p(z | \\xi_t) \\rightarrow p(z | \\xi_{t+1})$?\n",
    "\n",
    "$$\n",
    "p(z | \\xi_{t+1}) \\\\\n",
    "= p(z|\\xi_t, a_t, o_{t+1}) \\\\\n",
    "= p(z | \\xi_t)p(o_{t+1}|\\xi_t, a_t; z) / p(o_{t+1}|\\xi_t, a_t) \\\\\n",
    "= p(z|\\xi_t) p(o_{t+1}|o_t,a_t;z)/p(o_{t+1}|\\xi_t, a_t)\n",
    "$$\n",
    "\n",
    "Unfortunately, it is intractable to compute posterior since $p(o_{t+1}|\\xi_t, a_t) = \\int_z p(o_{t+1}|o_t,a_t;z) dz$. Instead, we can introduce a variational approximation $q(z | \\xi_t)$ and minimize ELBO:\n",
    "\n",
    "$$\n",
    "\\log p(\\xi_{t+1}) \\geq - \\text{KL}[q(z | \\xi_{t+1}) || p(z|\\xi_t)] + \\mathbb{E}_{q(z|\\xi_{t+1})} [\\log p(o_{t+1} | o_t, a_t; z)]\n",
    "$$\n",
    "\n",
    "Here, $q(z | \\xi_{t})$ can be implemented as a recurrent interaction encoder, and $p(o_{t+1} | o_t, a_t;z)$ is a forward dynamics model conditioned on latent physics variable $z$.\n",
    "\n",
    "$q$ and $p$ together compose a recurrent VAE.\n",
    "\n",
    "### Maximizing information gain\n",
    "Once we have learned approximate posterior $q$, we can use it to measure the information gain of an one-step interaction:\n",
    "\n",
    "$$\n",
    "\\text{IG} = H[q(z | \\xi_t)] - H[q(z | \\xi_t, a_t, o_{t+1})]\n",
    "$$\n",
    "\n",
    "where $H[\\cdot]$ is the entropy of a probability distribution.\n",
    "\n",
    "We would like our agent to choose actions that maximally reduce the uncertainty about the latent property $z$. Using the information gain as reward, we can use RL to find a policy that maximizes it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "I collected 20000 episodes of random interactions (100 interactions per episode) using the physics game. The physics properties "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Experience('data/record', obs_len=50)\n",
    "data_loader = DataLoader(dataset, batch_size=128,\n",
    "                         shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_size = 4\n",
    "action_size = 2\n",
    "hidden_size = 32\n",
    "latent_size = 4\n",
    "\n",
    "vae = VAE(obs_size, action_size, hidden_size, latent_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KLDivergence(mu1, logsigma1, mu2, logsigma2):\n",
    "    \"\"\" Compute KL(p1 || p2) where p1 ~ N(mu1, sigma1) and p2 ~ N(mu2, sigma2) \"\"\"\n",
    "    KLD = - 0.5 * torch.sum(1 + (2 * logsigma1)\\\n",
    "                            - (2 * logsigma2)\\\n",
    "                            - ((2 * logsigma1).exp() + (mu1 - mu2).pow(2)) / (2 * logsigma2).exp())\n",
    "\n",
    "    return KLD\n",
    "\n",
    "\n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logsigma):\n",
    "    \"\"\" VAE loss function \"\"\"\n",
    "    B, L, D = mu.size()\n",
    "    BCE = F.mse_loss(recon_x, x, size_average=False)\n",
    "\n",
    "    mu0 = torch.zeros((B, 1, D))\n",
    "    logsigma0 = torch.zeros((B, 1, D))\n",
    "\n",
    "    mu_prior = torch.cat((mu0, mu), dim=1)[:,:L,:]\n",
    "    logsigma_prior = torch.cat((logsigma0, logsigma), dim=1)[:,:L,:]\n",
    "\n",
    "    KLD = KLDivergence(mu, logsigma, mu_prior, logsigma_prior)\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/donlee/Research/WoodLab/env3.6/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/20000 (0%)]\tLoss: 89981.843750\n",
      "Train Epoch: 0 [60/20000 (13%)]\tLoss: 13730.121094\n",
      "Train Epoch: 0 [120/20000 (25%)]\tLoss: 9610.524089\n",
      "Train Epoch: 0 [180/20000 (38%)]\tLoss: 5700.190755\n",
      "Train Epoch: 0 [240/20000 (51%)]\tLoss: 6374.957031\n",
      "Train Epoch: 0 [300/20000 (64%)]\tLoss: 5629.203125\n",
      "Train Epoch: 0 [360/20000 (76%)]\tLoss: 7026.502604\n",
      "Train Epoch: 0 [420/20000 (89%)]\tLoss: 5298.242839\n",
      "Train Epoch: 1 [0/20000 (0%)]\tLoss: 6255.115234\n",
      "Train Epoch: 1 [60/20000 (13%)]\tLoss: 5762.320312\n",
      "Train Epoch: 1 [120/20000 (25%)]\tLoss: 4582.975260\n",
      "Train Epoch: 1 [180/20000 (38%)]\tLoss: 5321.255534\n",
      "Train Epoch: 1 [240/20000 (51%)]\tLoss: 5902.408203\n",
      "Train Epoch: 1 [300/20000 (64%)]\tLoss: 2452.852539\n",
      "Train Epoch: 1 [360/20000 (76%)]\tLoss: 2515.858887\n",
      "Train Epoch: 1 [420/20000 (89%)]\tLoss: 3015.258464\n",
      "Train Epoch: 2 [0/20000 (0%)]\tLoss: 3360.834310\n",
      "Train Epoch: 2 [60/20000 (13%)]\tLoss: 4067.247721\n",
      "Train Epoch: 2 [120/20000 (25%)]\tLoss: 2734.803711\n",
      "Train Epoch: 2 [180/20000 (38%)]\tLoss: 3018.711589\n",
      "Train Epoch: 2 [240/20000 (51%)]\tLoss: 2463.292318\n",
      "Train Epoch: 2 [300/20000 (64%)]\tLoss: 1939.989583\n",
      "Train Epoch: 2 [360/20000 (76%)]\tLoss: 1986.286784\n",
      "Train Epoch: 2 [420/20000 (89%)]\tLoss: 1829.576497\n",
      "Train Epoch: 3 [0/20000 (0%)]\tLoss: 1831.176758\n",
      "Train Epoch: 3 [60/20000 (13%)]\tLoss: 1635.582682\n",
      "Train Epoch: 3 [120/20000 (25%)]\tLoss: 1670.622396\n",
      "Train Epoch: 3 [180/20000 (38%)]\tLoss: 1599.181803\n",
      "Train Epoch: 3 [240/20000 (51%)]\tLoss: 1352.086589\n",
      "Train Epoch: 3 [300/20000 (64%)]\tLoss: 1396.304036\n",
      "Train Epoch: 3 [360/20000 (76%)]\tLoss: 1233.540283\n",
      "Train Epoch: 3 [420/20000 (89%)]\tLoss: 1347.325277\n",
      "Train Epoch: 4 [0/20000 (0%)]\tLoss: 1238.628581\n",
      "Train Epoch: 4 [60/20000 (13%)]\tLoss: 1481.163249\n",
      "Train Epoch: 4 [120/20000 (25%)]\tLoss: 1162.355876\n",
      "Train Epoch: 4 [180/20000 (38%)]\tLoss: 1372.929362\n",
      "Train Epoch: 4 [240/20000 (51%)]\tLoss: 1208.710286\n",
      "Train Epoch: 4 [300/20000 (64%)]\tLoss: 887.876058\n",
      "Train Epoch: 4 [360/20000 (76%)]\tLoss: 1356.421224\n",
      "Train Epoch: 4 [420/20000 (89%)]\tLoss: 1113.219564\n",
      "Train Epoch: 5 [0/20000 (0%)]\tLoss: 1390.287598\n",
      "Train Epoch: 5 [60/20000 (13%)]\tLoss: 1380.031250\n",
      "Train Epoch: 5 [120/20000 (25%)]\tLoss: 1207.998779\n",
      "Train Epoch: 5 [180/20000 (38%)]\tLoss: 1228.487630\n",
      "Train Epoch: 5 [240/20000 (51%)]\tLoss: 1011.486491\n",
      "Train Epoch: 5 [300/20000 (64%)]\tLoss: 1001.029378\n",
      "Train Epoch: 5 [360/20000 (76%)]\tLoss: 1129.551270\n",
      "Train Epoch: 5 [420/20000 (89%)]\tLoss: 1247.029948\n",
      "Train Epoch: 6 [0/20000 (0%)]\tLoss: 937.768311\n",
      "Train Epoch: 6 [60/20000 (13%)]\tLoss: 1212.212891\n",
      "Train Epoch: 6 [120/20000 (25%)]\tLoss: 1040.670166\n",
      "Train Epoch: 6 [180/20000 (38%)]\tLoss: 1116.798584\n",
      "Train Epoch: 6 [240/20000 (51%)]\tLoss: 1035.098714\n",
      "Train Epoch: 6 [300/20000 (64%)]\tLoss: 1159.195719\n",
      "Train Epoch: 6 [360/20000 (76%)]\tLoss: 1228.018229\n",
      "Train Epoch: 6 [420/20000 (89%)]\tLoss: 1001.254395\n",
      "Train Epoch: 7 [0/20000 (0%)]\tLoss: 1227.733724\n",
      "Train Epoch: 7 [60/20000 (13%)]\tLoss: 1117.070475\n",
      "Train Epoch: 7 [120/20000 (25%)]\tLoss: 1259.051351\n",
      "Train Epoch: 7 [180/20000 (38%)]\tLoss: 1073.191569\n",
      "Train Epoch: 7 [240/20000 (51%)]\tLoss: 931.854492\n",
      "Train Epoch: 7 [300/20000 (64%)]\tLoss: 1181.536296\n",
      "Train Epoch: 7 [360/20000 (76%)]\tLoss: 768.271973\n",
      "Train Epoch: 7 [420/20000 (89%)]\tLoss: 813.826660\n",
      "Train Epoch: 8 [0/20000 (0%)]\tLoss: 807.022217\n",
      "Train Epoch: 8 [60/20000 (13%)]\tLoss: 1050.829671\n",
      "Train Epoch: 8 [120/20000 (25%)]\tLoss: 986.306234\n",
      "Train Epoch: 8 [180/20000 (38%)]\tLoss: 864.933268\n",
      "Train Epoch: 8 [240/20000 (51%)]\tLoss: 1095.456462\n",
      "Train Epoch: 8 [300/20000 (64%)]\tLoss: 834.731527\n",
      "Train Epoch: 8 [360/20000 (76%)]\tLoss: 988.483480\n",
      "Train Epoch: 8 [420/20000 (89%)]\tLoss: 944.376058\n",
      "Train Epoch: 9 [0/20000 (0%)]\tLoss: 864.041748\n",
      "Train Epoch: 9 [60/20000 (13%)]\tLoss: 808.525879\n",
      "Train Epoch: 9 [120/20000 (25%)]\tLoss: 838.133301\n",
      "Train Epoch: 9 [180/20000 (38%)]\tLoss: 931.532959\n",
      "Train Epoch: 9 [240/20000 (51%)]\tLoss: 918.688070\n",
      "Train Epoch: 9 [300/20000 (64%)]\tLoss: 937.549967\n",
      "Train Epoch: 9 [360/20000 (76%)]\tLoss: 901.253418\n",
      "Train Epoch: 9 [420/20000 (89%)]\tLoss: 939.689779\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(vae.parameters())\n",
    "vae.train()\n",
    "for epoch in range(10):\n",
    "    for batch_idx, data in enumerate(data_loader):\n",
    "        o, a, o_next = data\n",
    "        optimizer.zero_grad()\n",
    "        o_pred, mu, logsigma = vae(o, a, o_next)\n",
    "        loss = loss_function(o_pred, o_next, mu, logsigma)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 20 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(data_loader.dataset),\n",
    "                    100. * batch_idx / len(data_loader),\n",
    "                    loss.item() / len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check\n",
    "Let's do a quick sanity check.\n",
    "First, sample an interaction history and encode it to get a distribution for latent variable $z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "o, a, o_next = dataset[0]\n",
    "o = o.unsqueeze(0)\n",
    "a = a.unsqueeze(0)\n",
    "o_next = o_next.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, logsigma = vae.encoder(o, a, o_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine how the belief changes over a series of interactions. We can see that the mean $\\mu$ starts with a value close to zero and converges to some value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2331, -0.2753,  0.3951,  0.2476],\n",
       "        [-0.3732, -0.4500,  0.6920, -0.0653],\n",
       "        [-0.3820, -0.5609,  0.9611, -0.1218],\n",
       "        [-0.3772, -0.7154,  1.1976, -0.1425],\n",
       "        [-0.4185, -0.8637,  1.3545, -0.1288],\n",
       "        [-0.4382, -0.9238,  1.4648, -0.1471],\n",
       "        [-0.5279, -1.1424,  1.5633, -0.2471],\n",
       "        [-0.5341, -1.1987,  1.6579, -0.2509],\n",
       "        [-0.5470, -1.2428,  1.7236, -0.2424],\n",
       "        [-0.5650, -1.3049,  1.7846, -0.2879],\n",
       "        [-0.5921, -1.3588,  1.8285, -0.2887],\n",
       "        [-0.6128, -1.3935,  1.8698, -0.2937],\n",
       "        [-0.6307, -1.4159,  1.9094, -0.3024],\n",
       "        [-0.6490, -1.4577,  1.9556, -0.3149],\n",
       "        [-0.6590, -1.4665,  1.9905, -0.3229],\n",
       "        [-0.6713, -1.4784,  2.0112, -0.3323],\n",
       "        [-0.6864, -1.5043,  2.0248, -0.3541],\n",
       "        [-0.7008, -1.5248,  2.0434, -0.3720],\n",
       "        [-0.7394, -1.5760,  2.0344, -0.4300],\n",
       "        [-0.7389, -1.5753,  2.0542, -0.4309],\n",
       "        [-0.7417, -1.5823,  2.0700, -0.4361],\n",
       "        [-0.7634, -1.6089,  2.0857, -0.4560],\n",
       "        [-0.7655, -1.6184,  2.0983, -0.4589],\n",
       "        [-0.7773, -1.6461,  2.1193, -0.4661],\n",
       "        [-0.7769, -1.6526,  2.1338, -0.4688],\n",
       "        [-0.7779, -1.6589,  2.1505, -0.4738],\n",
       "        [-0.8202, -1.7088,  2.1834, -0.5226],\n",
       "        [-0.8123, -1.7163,  2.1997, -0.5228],\n",
       "        [-0.8170, -1.7267,  2.2061, -0.5533],\n",
       "        [-0.8152, -1.7256,  2.2192, -0.5530],\n",
       "        [-0.8159, -1.7367,  2.2317, -0.5539],\n",
       "        [-0.8167, -1.7589,  2.2424, -0.5747],\n",
       "        [-0.8170, -1.7622,  2.2558, -0.5775],\n",
       "        [-0.8203, -1.7606,  2.2652, -0.5787],\n",
       "        [-0.8238, -1.7588,  2.2712, -0.5792],\n",
       "        [-0.8276, -1.7615,  2.2798, -0.5788],\n",
       "        [-0.8249, -1.7630,  2.2914, -0.5817],\n",
       "        [-0.8272, -1.7690,  2.2952, -0.5808],\n",
       "        [-0.8284, -1.7692,  2.3021, -0.5805],\n",
       "        [-0.8312, -1.7689,  2.3061, -0.5843],\n",
       "        [-0.8303, -1.7709,  2.3166, -0.5842],\n",
       "        [-0.8354, -1.7780,  2.3209, -0.5894],\n",
       "        [-0.8384, -1.7853,  2.3281, -0.5857],\n",
       "        [-0.8413, -1.7910,  2.3356, -0.5840],\n",
       "        [-0.8471, -1.8022,  2.3389, -0.5838],\n",
       "        [-0.8482, -1.8014,  2.3446, -0.5854],\n",
       "        [-0.8498, -1.7980,  2.3512, -0.5866],\n",
       "        [-0.8513, -1.8004,  2.3614, -0.5906],\n",
       "        [-0.8588, -1.8045,  2.3620, -0.5940]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance $\\sigma$ starts from a value close to 1 (similar to prior dist.) and slowly decreases over time. This means that the agent is becoming more confident about its belief, as it gains more observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8698, 0.7583, 0.9630, 0.9691],\n",
       "        [0.7720, 0.6687, 0.9080, 0.8042],\n",
       "        [0.7423, 0.6072, 0.8732, 0.7616],\n",
       "        [0.7253, 0.5392, 0.8489, 0.7420],\n",
       "        [0.6892, 0.4881, 0.8084, 0.7170],\n",
       "        [0.6586, 0.4565, 0.7772, 0.6817],\n",
       "        [0.6209, 0.3807, 0.7152, 0.6428],\n",
       "        [0.5963, 0.3581, 0.6910, 0.6200],\n",
       "        [0.5780, 0.3403, 0.6682, 0.6024],\n",
       "        [0.5631, 0.3189, 0.6514, 0.5749],\n",
       "        [0.5450, 0.3017, 0.6260, 0.5594],\n",
       "        [0.5293, 0.2870, 0.6036, 0.5454],\n",
       "        [0.5201, 0.2739, 0.5885, 0.5325],\n",
       "        [0.5001, 0.2596, 0.5679, 0.5149],\n",
       "        [0.4941, 0.2467, 0.5581, 0.4995],\n",
       "        [0.4867, 0.2395, 0.5484, 0.4874],\n",
       "        [0.4755, 0.2320, 0.5371, 0.4720],\n",
       "        [0.4700, 0.2252, 0.5285, 0.4628],\n",
       "        [0.4498, 0.2128, 0.5129, 0.4295],\n",
       "        [0.4444, 0.2086, 0.5044, 0.4247],\n",
       "        [0.4403, 0.2051, 0.4979, 0.4200],\n",
       "        [0.4308, 0.1984, 0.4893, 0.4086],\n",
       "        [0.4265, 0.1957, 0.4836, 0.4043],\n",
       "        [0.4181, 0.1905, 0.4746, 0.3983],\n",
       "        [0.4150, 0.1881, 0.4698, 0.3952],\n",
       "        [0.4116, 0.1851, 0.4639, 0.3924],\n",
       "        [0.4066, 0.1805, 0.4634, 0.3869],\n",
       "        [0.4049, 0.1791, 0.4599, 0.3853],\n",
       "        [0.4017, 0.1747, 0.4554, 0.3755],\n",
       "        [0.4002, 0.1731, 0.4509, 0.3739],\n",
       "        [0.3979, 0.1716, 0.4473, 0.3721],\n",
       "        [0.3965, 0.1680, 0.4439, 0.3683],\n",
       "        [0.3951, 0.1665, 0.4409, 0.3671],\n",
       "        [0.3946, 0.1650, 0.4387, 0.3654],\n",
       "        [0.3939, 0.1641, 0.4370, 0.3634],\n",
       "        [0.3932, 0.1633, 0.4353, 0.3623],\n",
       "        [0.3920, 0.1622, 0.4339, 0.3615],\n",
       "        [0.3907, 0.1608, 0.4307, 0.3604],\n",
       "        [0.3903, 0.1599, 0.4291, 0.3593],\n",
       "        [0.3900, 0.1591, 0.4280, 0.3575],\n",
       "        [0.3891, 0.1583, 0.4264, 0.3570],\n",
       "        [0.3876, 0.1573, 0.4250, 0.3553],\n",
       "        [0.3865, 0.1564, 0.4229, 0.3546],\n",
       "        [0.3854, 0.1557, 0.4213, 0.3539],\n",
       "        [0.3835, 0.1540, 0.4175, 0.3528],\n",
       "        [0.3830, 0.1530, 0.4158, 0.3521],\n",
       "        [0.3829, 0.1523, 0.4151, 0.3514],\n",
       "        [0.3817, 0.1512, 0.4143, 0.3508],\n",
       "        [0.3798, 0.1499, 0.4131, 0.3480]], grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logsigma[0].exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
